#!/usr/bin/perl

########################################################
#
# This is the configuration file for the srna-
# tools application.
# This file configures the application itself. There
# is another config file called tools.conf, which
# configures tool-specific behaviour.
# 
# The format is normal Perl code that is interpreted by
# the ConifgAuto module in the SrnaTools::Config class.
#
########################################################

use strict;

my %cnf=(
  
  # These are the tools that we make available on this site
  # Use short lowercase names here that can also be used to
  # pass between scripts on the website etc. A display name
  # and description can be defined in the tools section
  tools => [
    'mircat',
    'siloco',
    'phasing',
    'adaptor',
    'mirprof',
    'filter',
    'siloma',
    'hp_tool',
    'firepat',
  ] ,
  
  # Set debug to 1 to print full error
  # messages including script names and line numbers
  # in the browser. Set this to 0 for live web sites.
  # In both cases, full error messages are printed to 
  # the server's error-log in addition.
  #debug => 1,
  # if no_job_submission is used, jobs are created
  # and (batch jobs) sent to queue dir but are not
  # submitted to queue. This can be useful for 
  # debugging purposes
  #no_job_submission => 1,

  ################################################
  # Paths to directories on server and cluster
  # where we put jobs into the queue, store
  # genome data and third-party binaries.
  # All paths are relative to the "doc root"
  # i.e. the srna-tools directory
  # TODO some of these should be set as default
  # so we only need to overwrite if we don't use
  # standard dir structure
  ################################################
  
  # path to srna-tools "root directory" on the 
  # cluster backend and the server. The server root
  # could be determined dynamically at run time but
  # with this set up we can share resources between
  # web apps by pointing them to the same root dir.
  # It also creates a more unified interface to path data
  root_dir => {
    #cluster => '/gpfs/sys/srna-tools/scratch/',
    cluster => '/gpfs/sys/srna-tools/scratch/development/frank_animal/',
    server => '/usr/local/apache2/htdocs/srna-tools/frank_animal/',
    local_queue => '/usr/local/apache2/htdocs/srna-tools/frank_animal/',   
  },

  
  # this is where jobs are send to get queued,
  # not the job storage area on the server
  # (job_dir_server). 
  # We give a value for environmet "server" 
  # just for instant jobs where storage = 
  # execution dir.
  queue_job_dir => {
    cluster => '/jobs/',
    local_queue => '/jobs_for_batch_queue/',
    server => '/jobs/',
  },
  
  # Dir for uploaded and finished jobs (not queued ones)
  # This is where jobs get created and where we copy the
  # results to after they have been completed. We only 
  # have this on the server.
  job_storage_dir => {
    server =>  '/jobs/'
  }, 

  # path to backend scripts
  lib_dir => {
    server  => '/lib/',
    cluster => '/lib/',
    local_queue => '/lib/'
  },
  
  # additional search paths for Perl
  # modules in different environments
  module_dir => {
    cluster => [ 
      "/usr/local/Bioperl/lib/perl5/site_perl/5.8.3", 
      "/usr/lib/perl5/site_perl/5.8.3"
    ]
  },
  
  # public data, such as genomes
  data_dir => {
    server  => '/data/', 
    cluster => '/data/',
    local_queue => '/data/'
  },

  log_file => { 
    server  => '/log/batch_jobs.log',
    cluster => '/log/batch_jobs.log',
    local_queue => '/log/batch_jobs.log',
  },

  # These are the user-name@host-name strings
  # that we need in order to ssh to remote machines.
  # 'cluster' will be used on the server to send
  # data to the cluster and 'server' is only
  # needed on the cluster to send data back.
  user_host_name => { 
    cluster => 'srna-tools@escluster.uea.ac.uk',
    server => 'apache@fschwachpc2.cmp.uea.ac.uk'
  },
  
  # total disk usage allowance in byte (job queue directories)
  # stop accepting jobs if this maximum has been reached.
  usage_allowance => {
    server  => 20e9,
    local_queue => 20e9,
    cluster => 500e9   
  },
  
########################################################
# web section
# Configuration in this section is specific to web 
# implementations of the tools
########################################################

  # Header and page title text
  title => 'animal version',
  
  # Some common html partials, such as the index,
  # about, publications and the sidenav, which is
  # included in the layout.
  home_page_partial => 'start.tt',
  sidenav_partial => 'sidenav.tt',

  # set maximum amount of POST data to X *1000
  # where X is in MB
  post_max => 200 * 1e6,

  # This is the email address we send 
  # confimration emails from
  email_from => 'no-reply-srna-tools@cmp.uea.ac.uk',
  
  # Admin email address
  admin_email => 'srna-tools@cmp.uea.ac.uk',

  # The path and filename to the downtime
  # schedule file
  downtime_schedule_file =>  '/config/downtime_schedule.txt',

  # This is where the html templates (html partials) live
  html_template_path => 'html_templates/',
  
  # Templaes for emails
  email_template_path => 'email_templates/',
  
  ########################################################
  # Command Line Interface section
  ########################################################
  cli_template_path => 'cli_templates/',
  
) ;



# DO NOT DELETE THIS
\%cnf ;
